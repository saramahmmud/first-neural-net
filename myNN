import numpy as np

# this is a supervised regression problem
# supervised becaue our examples have inputs and outputs
# regression beaucse output is continuous (pecentage rather than grade letter A, B C)


# x = (hours sleeping, hours studying), y = Score on test
xAll = np.array(([3,5], [5,1], [10,2]), dtype = float)
y = np.array(([75], [82]), dtype = float)

# Scale data by dividing by max value
xAll = xAll/np.amax(xAll, axis = 0) # max hours sleeping is 10?
y = y/100 #max test score is 100%

# split data
X = xAll[:2] # training data
xPredicted = xAll[-1:] # testing data



# Create a class for the NN
class Neural_Network(object):
    def __init__(self):
        # Define hyperparameters (no. of nodes and synapses)
        #hyperparameters are constants that establish the structure and behaviour of the network
        #hyperparameters cannot update themselves i.e. cannot create a new hidden layer, we must decide the no. of layers
        #hyperparamteres can only update their weights
        self.inputLayerSize = 2 #"self" makes variables accessible to whole class
        self.outputLayerSize = 1
        self.hiddenLayerSize = 3

        # randomly generste weights
        self.w1 = np.random.randn(self.inputLayerSize, self.hiddenLayerSize) # 2x3 weight matrix
        self.w2 = np.random.randn(self.hiddenLayerSize, self.outputLayerSize) # 3x1 weight matrix

    # Create function for forward propogation

    # Use matrices to pass through mulitple inputs at once- big computational speedups
    # Input data matrix x has dimesion 3x2
    # Weights matrix has diemntion 2x3
    # Second layer had dimesnions 3x3
    # [3 5]  * [W11 W12 W13]   =   [3
    # [5 1]    [W21 W22 W23]            by
    # [10 2]                                3}
    # x         W1                 z2
    # z2 = x * W1

    # Then you apply the activation function (sigmoid) to each element in the 3x3 matrix z2
    # a2 = f(z2)

    # Mulitply this activated layer by a secon set of weights
    # second weight matrix, w2 has dimensions 3x1
    # a2 * w2 = z3
    # z3 has dimesions 3x1

    # Finally, apply activation function to z3 to get output, ŷ (yHat)
    # ŷ = f(z3)
    # ŷ has dimesnions 3x1  (each rown of input gives one output)

    def forward(self, x):
        # Propogate inputs through network
        self.z2 = np.dot(x, self.w1) # z2 = x * W1
        self.a2 = self.sigmoid(self.z2) # a2 = f(z2)
        self.z3 = np.dot(self.a2, self.w2) # z3 =a2 * w2
        yHat = self.sigmoid(self.z3) # yHat = f(z3)
        return yHat


    def sigmoid(self, z):
        #Apply sigmoid activation funciton
        return 1/(1+np.exp(-z))
        #a2 = f(z2) where f is activation functino
        # a2 is same dimension as z2, 3x3

    # Training a NN = minimising loss function
    # overall loss, J = Σ (1/2 * (y-ŷ)^2)
    # Use gradient descent to find weights for which loss is minimum
    # Batch greadient descent because we use the sum of the squares of the losses
    # quadratic functions are convex so no local/global minima to worry about

    #gradient = dJ/dW
    #2 sets of weights so lets compute both dJ/dW1 and dJ/dW2
    # these matrices have the same dimensions as W1 and W2 as we differentiate each term

    # dJ/dW2
    # = d Σ (1/2 * (y-ŷ)^2) / (dW2)
    # Sum rule in defferentiation- derivative of a sum =  sum of derivatives
    # Therefore we forget about Σ for now
    # d (1/2 * (y-ŷ)^2) / (dW2), multiply by exponent and terake 1 from exponent
    # (y-ŷ) then apply chain rule
    # y is a constant so derivative = 0
    # ŷ does change with respect to W2, so by chain rule, multiply by derivative of -ŷ
    # = - (y-ŷ) dŷ/dW2
    # we know that ŷ = f(z3)
    # dz/dx = dz/dy * dy/dx
    # therefore dŷ/dW2 = dŷ/ dz3 * dz3 /dW2

    # differentiating sigmoid activation fucntion with respect to z
    # f(z) = 1/ (1 + e^-z)
    # If f(z) = u/v then f'(z) = u'v - v'u/ (v^2)
    # f'(z) = 0*v - (1* e^-z) / (1 + e^-z)^2
    # f'(z) = e^-z /(1 + e^-z)^2

    def sigmoidPrime (self, s):
        #Derivative of sigmoid fucntion, two ways:
        # return np.exp(-z)/((1+ np.exp(-z))**2)
        return s * (1-s)
        # derivative is largest where sigmoid is steepest - ie around 0

    # so dŷ/ dz3 = f'(z3)
    # z3 = a2*w2
    # Therefore dz3/dW2 = a2 for that synapse

    def backward (self, X, y, yHat):
        #derivative with respect to W1 and W2
        self.yHat_error = y - yHat
        self.yHat_delta = self.yHat_error * self.sigmoidPrime(yHat)

        self.a2_error = self.yHat_delta.dot(self.w2.T)
        self.a2_delta = self.a2_error * self.sigmoidPrime(self.a2)

        self.w1 += X.T.dot(self.a2_delta)
        self.w2 += self.a2.T.dot(self.yHat_delta)

    def train(self, X, y):
        yHat = self.forward(X)
        self.backward(X, y, yHat)

    def predict(self):
      print ("Predicted data based on trained weights:");
      print ("Input (scaled): \n" + str(xPredicted));
      print ("Output: \n" + str(self.forward(xPredicted)));


NN = Neural_Network()
for i in range(3000): # trains the NN 1,000 times
  print ("# " + str(i) + "\n")
  print ("Input (scaled): \n" + str(X))
  print ("Actual Output: \n" + str(y))
  print ("Predicted Output: \n" + str(NN.forward(X)))
  print ("Loss: \n" + str(np.mean(np.square(y - NN.forward(X)))) )# mean sum squared loss
  print ("\n")
  NN.train(X, y)


NN.predict()
